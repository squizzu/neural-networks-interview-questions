# Squizzu ‚Äì Neural Networks Interview questions üß†‚ö°

<p align="center">
  <img src="./assets/squizzu-landing-screen.png" alt="Squizzu landing"/>
</p>

<p align="center">
  üëâ <a href="https://squizzu.com"><strong>Try full quizzes on squizzu.com</strong></a>
</p>

---

## üìö About this repository

This repository contains **Neural Networks interview questions** taken from sections available on **[Squizzu](https://squizzu.com)**.

The goal is to:
- showcase the **quality and style** of Squizzu questions
- provide **interview-oriented practice**
- encourage deeper learning instead of memorization

‚ùó **Correct answers are intentionally not provided here**.  
To unlock full explanations and verified answers, visit **squizzu.com**.

---

## üß© Neural Networks Interview Questions

### 1Ô∏è‚É£ Do you need to normalize inputs before training in neural networks?

- A) Yes, because it ensures faster convergence
- B) No, it doesn't affect the training process
- C) Yes, because it avoids potential numerical issues
- D) No, normalization is only required in image processing tasks

---

### 2Ô∏è‚É£ How does bias function within a neural network?

- A) It adjusts the output along with the weighted input to improve model's predictions.
- B) It scales all input values by a fixed factor before passing them through the activation function.
- C) It reduces the dimensionality of input data prior to processing.
- D) It prevents overfitting by adding random noise to input values during training.

---

### 3Ô∏è‚É£ In which layer is the ReLU activation function usually used?

- A) Input Layer
- B) Hidden Layer
- C) Output Layer
- D) Normalization Layer

---

### 4Ô∏è‚É£ What is the vanishing gradient problem?

- A) When gradients become too small to update the network effectively during training.
- B) When gradients become too large causing the network weights to change erratically during training.
- C) When gradients stabilize and do not change much after a few epochs of training.
- D) When gradients oscillate between positive and negative values leading to slow convergence.

---

### 5Ô∏è‚É£ Assume that dropout rate is equal to 0.7. What does it mean?

- A) 70% of the neurons are dropped during training.
- B) 70% of the neurons are retained during training.
- C) 70% of the neurons are dropped during inference.
- D) 70% of the neurons are retained during inference.

